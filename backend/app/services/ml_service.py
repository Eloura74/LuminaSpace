import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import os

# --- Configuration ML ---
# Utilisation de mod√®les optimis√©s pour la vitesse/m√©moire si possible
# "lllyasviel/sd-controlnet-canny" est le standard pour ControlNet Canny 1.5
CONTROLNET_ID = "lllyasviel/sd-controlnet-canny"
MODEL_ID = "runwayml/stable-diffusion-v1-5" 

class MLService:
    def __init__(self):
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üöÄ ML Service initialized on {self.device}")

    def load_model(self):
        """Charge le mod√®le Stable Diffusion + ControlNet en m√©moire."""
        if self.pipe is not None:
            return self.pipe

        print("‚è≥ Loading Stable Diffusion & ControlNet...")
        try:
            controlnet = ControlNetModel.from_pretrained(
                CONTROLNET_ID, 
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            
            self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
                MODEL_ID, 
                controlnet=controlnet, 
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                safety_checker=None # D√©sactiv√© pour la vitesse et √©viter les faux positifs
            )

            # Optimisation Scheduler
            self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)

            # Optimisation M√©moire
            if self.device == "cuda":
                self.pipe.enable_model_cpu_offload() # Tr√®s efficace pour √©conomiser la VRAM
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                    print("‚úÖ xformers enabled for memory efficient attention")
                except Exception as e:
                    print(f"‚ö†Ô∏è xformers not available: {e}")
            
            self.pipe.to(self.device)
            self.log_gpu_info()
            print("‚úÖ Model loaded successfully!")
            return self.pipe
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            return None

    def log_gpu_info(self):
        """Affiche les informations sur le GPU."""
        if torch.cuda.is_available():
            print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
            print(f"üíæ VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            print(f"üíæ VRAM Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
        else:
            print("üíª Running on CPU")

    def generate(self, prompt, image, negative_prompt="", steps=20, guidance_scale=7.5):
        """G√©n√®re une image √† partir d'un prompt et d'une image de contr√¥le (Canny)."""
        if self.pipe is None:
            self.load_model()
        
        # G√©n√©ration
        output = self.pipe(
            prompt,
            image=image,
            negative_prompt=negative_prompt,
            num_inference_steps=steps,
            guidance_scale=guidance_scale
        )
        
        return output.images[0]

# Singleton instance
ml_service = MLService()

# --- Inpainting Pipeline ---
from diffusers import StableDiffusionInpaintPipeline

class InpaintingService:
    def __init__(self):
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üé® Inpainting Service initialized on {self.device}")

    def load_model(self):
        if self.pipe is not None:
            return self.pipe
        
        print("‚è≥ Loading Inpainting Model...")
        try:
            self.pipe = StableDiffusionInpaintPipeline.from_pretrained(
                "runwayml/stable-diffusion-inpainting",
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                safety_checker=None
            )
            
            # Optimisations
            self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)
            if self.device == "cuda":
                self.pipe.enable_model_cpu_offload()
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                except:
                    pass
            
            self.pipe.to(self.device)
            print("‚úÖ Inpainting Model loaded!")
            return self.pipe
        except Exception as e:
            print(f"‚ùå Error loading inpainting model: {e}")
            return None

    def inpaint(self, prompt, image, mask_image, negative_prompt="", steps=20, guidance_scale=7.5):
        if self.pipe is None:
            self.load_model()
        
        output = self.pipe(
            prompt=prompt,
            image=image,
            mask_image=mask_image,
            negative_prompt=negative_prompt,
            num_inference_steps=steps,
            guidance_scale=guidance_scale
        )
        return output.images[0]

inpainting_service = InpaintingService()
